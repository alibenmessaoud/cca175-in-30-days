# CCA-175: Chapter 6

### Apache Spark

- Apache Spark is a fast and general engine for large scale data processing
- Written Scala, supports apps written in Java, Python, Scala
- Provides REPL shell for Python pyspark and Scala spark-shell

### RDD

- Fundamental unit of data in spark, spark programming consists of performing ops on RDDs 
- RDD is:
  - Resilient: can be lost in memory but it can recreated
  - Distributed: Processed across the cluster
  - Dataset: Data created or read.02323
- Create RDD by reading files, read data from memory, or another RDD
- RDD ops are:


- ```$ val data = sc.textFile("a.txt");``` takes file and create an RDD of separated lines; 
- ```$ data.count(); ``` returns a number;

### RDD operations

- Actions: action returns a value
  - count() number of elements
  - take(n) returns array of n first element
  - collect() returns array of elements
  - saveAsTextFile(dir) saves to a text file(s)
- Transformations: transformations defines an new RDD (immutable)
  - map(f)
    - Python: map(lambda l: l.toUpperCase()) 
    - Scala: map(l => l.toUpperCase)
  - filter(f)
    - Python: filter(lambda l: l.startsWith("K"))
    - Scala: filter(l => l.startsWith("K"))
- RDD is not processed until an action is performed
  - Example: read a file, map() all lines toUpperCase, filter() for lines containing K, count()!
  - Before count(), we have only transformations (map, filter), so, when count() is in its way to be called, all transformation will be applied

### Essentials

- â€‹