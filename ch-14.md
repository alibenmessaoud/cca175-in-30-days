# CCA-175: Chapter 14

### DataFrames and Apache Spark SQL

- Spark SQL?

  - module to process structured data, replace shark (deprecated), built on top of core
  - API to work with dat as tables: DataFrame + SQL engine

- SQL Context

  - Main entry to get SQL context

  - 2 impls: 

    - SQLContext (basic) 
    - HiveContext (recommended)
      - RW Hive/ HCatalog tables directly
      - Support HiveQL
      - Requires link between app and Hive APIs

  - Create how?

    ```scala
    from pyspark.sql import HiveContext
    sqlContext = HiveContext(sc)
    
    import org.apache.spark.sql.hive.HiveContext
    val sqlContext = new HiveContext(sc)
    import sqlContext.implicits._
    ```

- DataFrames

  - Main abstraction in Spark SQL ~ RDD in core Spark

  - Distributed collection of data organized into named columns

  - Built on a base RDD containing row objects

  - DataFrames are immutable 

  - Created from:

    - Automatically:

      - an existing structured data source: Hive table, Parquet file, JSON, JDBC, ORC	

        ```scala
        peopleDF = sqlContext.read.json("people.json");
        customerDF = sqlContext.read.table("customers");
        // parquet(filename)
        // orc(filename)
        // jdbc(url,table,options)
        ```

    - Manually

      - specify settings for the DataFrameReader.read : format, options, schema

        ```
        sqlContext.read
        		 .format("com.databricks.spark.avro")
                  .load("/loudacre/accounts_avro")
                  
        sqlContext.read
                  .format("jdbc")
                  .option("url","jdbc:mysql://localhost/loudacre")
                  .option("dbtable","accounts")
                  .option("user","training")
                  .option("password","training")
                  .load()
        ```

      - Also: CSV, MySQL, Avro

    - an existing RDD

    - Operating on other DF

    - Defining programmatically a Schema

  - Transform and query

    - basic operations: 

      - schema to return schema, 

      - printSchema

      - cache/ persist to persist DF to disk or memory

      - columns returns names

      - dtypes return array of pairs name-type

        ```python
        peopleDF = sqlContext.read.json("people.json")
        for item in peopleDF.dtypes: print item
        ('age', 'bigint')
        ('name', 'string')
        ('pcode', 'string')
        
        val peopleDF = sqlContext.read.json("people.json")
        peopleDF.dtypes.foreach(println)
        (age,LongType)
        (name,StringType)
        (pcode,StringType)
        ```

      - explain prints debug info

    - Queries: create a new DF, analogous to RDD transformations

      - distinct return new DF with distinct elements in this DF

      - join(df, [[col[, type]], [cnd]])

        - Type: inner (default), outer, left_outer, right_outer, or leftsemi

        ```scala
        peopleDF.join(pcodesDF, "pcode") // no shared code rows will be removed
        
        // Scala
        peopleDF.join(pcodesDF, Array("pcode"), "left_outer")
        // Python
        peopleDF.join(pcodesDF, "pcode", "left_outer")
        ```

        ```scala
        // Scala
        peopleDF.join(zcodesDF, $"pcode" === $"zip")
        // Python
        peopleDF.join(zcodesDF, peopleDF.pcode == zcodesDF.zip)
        // but it creates the same column with the name pcode and zip
        ```

        

      - limit: peopleDF.limit(3).show() 

      - select(col...): 

        ```
        peopleDF.select("age")
        peopleDF.select("name", "age")
        // Python
        peopleDF.select(peopleDF['age']) 
        peopleDF.select(peopleDF.age)
        peopleDF.select(peopleDF['name'],peopleDF['age']+10)
        
        //Scala
        peopleDF.select(peopleDF("age"))
        peopleDF.select($"age") 
        peopleDF.select(peopleDF("name"),peopleDF("age")+10)
        ```

      - where(cnd...) // filter: peopleDF.where("age>21")

      - sort(col...)

        - sort(peopleDF("age").desc) // peopleDF.sort(peopleDF("age").desc) 

      - groupBy, orderBy, and agg 

      - unionAll, and intersect 

      - avg, sampleBy, corr, and cov

      - rollup and cube

    - Actions: returns data to the driver

      - collect: returns all rows as an array of Row object
      - take(n)
      - count: peopleDF.count() gives 3
      - show(n)

  - SQL queries

    - We can use SQL queries with HiveContext

      - sqlContext. sql("""SELECT * FROM customers WHERE name LIKE "A%" """) 

    - Perform query on DF

      - peopleDF.registerTempTable("people") 

    - Query directly from JSON or Parquet files

      - sqlContext. sql("""SELECT * FROM json.'/user/training/people.json' 

        WHERE name LIKE "A%" """) 

  - Save DF

    - Use DataFrame.write to create a DataFrameWriter: Hive table, Parquet file, JSON, JDBC, ORC, Text:

      - peopleDF.write.saveAsTable("people") 

    - DataFrameWriter options

      - format

      - mode: overwrite, append, ignore or error (default is error)

      - partitionBy: stores data in partitioned directories in the form column=value (as with Hive/Impala partitioning)

      - options: properties for the target data source

      - save

        ```
        peopleDF.write
                .format("parquet")
                .mode("append")
                .partitionBy("age")
                .saveAsTable("people")
        ```

  - DF and RDD

    - Base RDD contains Row object: peopleRDD = peopleDF.rdd 

      - table goes [Row[val, ...]]
      - Row RDD have all the standard Spark actions and Tx, can be pair-RDD for MR

    - DF provide method such as map, flatMap, and foreach for converting to RDDs

    - Extracting data from Row Object depends on language

      - Python: names are attributes: 

        - row.age 

          ```python
          peopleRDD = peopleDF.map(lambda row:(row.pcode,row.name))
          peopleByPCode = peopleRDD.groupByKey()
          ```

      - Scala: use Array-like syntax

        - row(n) 

        - row.fieldIndex("age") 

        - row.getAs[ Long ] ("age") 

        - row.getString(n)  

          ```
          val peopleRDD = peopleDF.map(row =>
          	(
          		row(row.fieldIndex("pcode")), row(row.fieldIndex("name"))
          	)
          )
          val peopleByPCode = peopleRDD.groupByKey()
          ```

    - RDD to DF

      - createDataFrame 

        ```python
        schema = StructType([StructField("age",IntegerType(),True),
        					StructField("name",StringType(),True),
                            StructField("pcode",StringType(),True)])
        myrdd = sc.parallelize([(40,"Abram","01601"), (16,"Lucia","87501")])
        mydf = sqlContext.createDataFrame(myrdd, schema)
        ```

  -  Impala vs SSQL

    - SSQL built on Spark, a general purpose processing engine
    - Impala is a specialized SQL engine: better perf, more mature, secure with sentry
    - SSQL is good for ETL and access to structured data required by a Spark App
    - Impala is good for data analysis, interactive queries

  - Hive on Spark vs SSQL

    - SSQL provides DF API to process structured data, mix SQL with procedural process
    - Hive provides SQL abstraction over MR or Spark for  non programmers

### Essentials

- Spark SQL is a Spark API for handling structured and semi-structured data 
- Entry point is a SQL context ยง DataFrames are the key unit of data 
  - DataFrames are based on an underlying RDD of Row objects 
  - DataFrames query methods return new DataFrames; similar to RDD transformations
  - The full Spark API can be used with Spark SQL data by accessing the underlying RDD 
- Spark SQL is not a replacement for a database, or a specialized SQL engine like Impala 
  - Spark SQL is most useful for ETL or incorporating structured data into a Spark application