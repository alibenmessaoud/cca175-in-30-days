# CCA-175: Chapter 10

### Spark Application Configuration 

- Configuring apps 

  - Declaratively at the submit-spark command:

    - spark.master
    - spark.app.dir: used tp store local file as in shuffle operation (/tmp)
    - spark.ui.port: used to run the Spark app UI (default 4040)
    - spark.executor.memory: memory to allocate to each Executor (default 1g)
    - spark.driver.memory: memory to allocate to the driver in client mode (default 1g)

    ```
    spark-submit --conf spark.executor.cores=4
    ```

  - Programmatically 

    - Spark configuration settings are part of the Spark context
    - setAppName(name)
    - setMaster(master) 
    - set(property-name, value) : SparkConf 

    ```
    val sconf = new SparkConf()
    .setAppName("Word Count")
    .set("spark.ui.port","4141")
    val sc = new SparkContext(sconf)
    ```

  - View the Spark settings in the Spark UI App > Environment menu

- Logging

  - Spark uses Apache Log4j for logging
  - Log4j provides several logging levels – TRACE – DEBUG – INFO – WARN – ERROR – FATAL – OFF
  - Log file locations depend on your cluster management platform
    - Yarn: 
      - stored on /var/log/hadoop-yarn if it is on
      - stored locally on each worker node if it off
  - Configure Log
    - SPARK_HOME/conf/log4j.properties* 
    - /usr/lib/spark/conf 
    - sc.setLogLevel("ERROR") 

### Essentials

- Spark configuration parameters can be set declaratively using the spark-submit script or a properties file, or set programmatically using a SparkConf object
- Spark uses Log4j for logging
  - Configure using a log4j.properties file or sc.setLogLevel