# CCA-175: Chapter 3

### Hadoop Architecture:

- Cluster:  a set of machines. It provides storage, process & management of resources
- Node: a machine in a cluster
- Master: a node that manages work and data process
- Daemon: a program running in a node

=> Cluster = {Resources Manager + Process + Storage}

### Storage:

HDFS:

- FS written in Java, based on Google FS, sits on top of native FS, provides redundant storage

  - HDFS => OSFS => Hard drive

- Best performance increase with millions and large files

- Files are written once so no random write

- Optimized for large reads and streaming, so no random reads

- Split files to 128 MB by default, distribute at load time and replicate on multiple Data Nodes (3x)

- Name Node stores the locations of files and blocks (metadata)

- Example:

  - f1.txt goes to 3 blocks 1, 2, 3

  - f2.txt goes to 2 blocks 4, 5

  - Cluster nodes are A, B, C, D, E

  - Storing goes as A(1, 3, 4), B(1, 2, 3, 4), C(3, 5), D(1, 2, 5), E(2, 4, 5)

  - Name nodes stores : 

    f1.txt => 1, 2, 3; f2.txt => 4, 5

    1 => A, B, D; 2 => B, D, E; 3 => A, B, C; 4 => A, B, E; 5 => C, E, D

- Name node run all the time. If the Name Node stops, the cluster becomes inaccessible. HDFS is set known for high availability! => Two Name Nodes: Active and Standby : Standby Name Node takes over automatically if Active Name Node fails.

	 Options for Accessing	HDFS: commands, Spark by URI, APIs (Sqoop, Flume, etc), REST (Hue)

- HDFS Recommendations: HDFS is a repository for your data => Structure and organize carefully


### File formats

- The tools in Hadoop ecosystem uses many file formats (Text, Sequence, Avro, Parquet)

  - Use cases need specific format
  - You can define your format
  - HDFS considers files to be simply a sequence of bytes 

- Types:

  - Text: 

    - Most basic, uses comma and tabs, compatible with many apps
    - Human readable text, poor performance when converting types

  - Sequence

    - Store key-value in binary format, better performance, better than text files
    - Can store images, poor interoperability

  - Avro

    - Embed Schema turns data readable, best choice for general storage

    - Optimized binary encoding, can be used outside Hadoop, long-term storage

    - Excellent performance et interoperability

      ```
      $ avro-tools tojson f.avro
      $ avro-tools getschema f.avro
      ```

  - Parquet

    - Columnar storage, data stored by column not by row, embed Schema

    - Efficient when selecting a subset of some columns, reduces storage space, increases performance

    - Excellent interoperability and performance

      ```
      $ parquet-tools head f.parqet
      $ parquet-tools schema f.parqet
      ```

- Compression

  - BZip2 saves most space
  - LZ4 & Snappy are much faster
  - Impala supports Snappy

### Essentials

HDFS is main storage layer

HDFS chunks data into blocks before storing in a cluster

Clusters are managed by NN running on MN

Access HDFS by commands, Hue, APIs

Many file formats 