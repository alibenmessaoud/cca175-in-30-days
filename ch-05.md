# CCA-175: Chapter 5

### Sqoop

- SQL-to-Hadoop, Apache project, originated by Cloudera
- Client-side app, CLI utility, uses Hadoop Map Reduce


- Import/ Export data between a database and HDFS

- Import can be in many formats: import all tables, single table, part of a table

- Import in 3-steps: examine tables, create and submit job to a cluster, fetch records and write in HDFS

  ```
  $ sqoop list-tables
    --vonnect jdbc:mysql://host/dbname
    --username user
    --password pwd
  ```

  ```
  $ sqoop eval
    --query "select * from my_table limit 3"
    --vonnect jdbc:mysql://host/dbname
    --username user
    --password pwd
  ```

### Importing

- Import is performed using MR jobs: RDBMS -> HDFS
- Sqoop examines the table to be imported
  - Determines the primary key
  - Runs boundary query to check number of rows
  - Divides result of boundary query by the number of tasks (mappers): equal loads for tasks
- Sqoop generates s Java sources: compiles and executes for the import process
- CLI options:
  - sqoop import-all-tables stores at ~, use "," as separation, subdirectoried by tables 
    - --warehouse-dir /folder-name to specify an other directory
  - sqoop import --table users 
    - --columns "id, state" 
    - --where "state=true"
    - --target-dir /path
    - --fields-terminated-by "\lol" instead of ","
    - --compress-codec org.apache.hadoop.io.compress.SnappyCodec
    - --asparquetfile | --as-avrodatafile

### Exporting

- HDFS -> RDBMS
- Useful for batch processing, export results for access by other systems
- Tables need to be exist before export
- CLI options:
  - sqoop export
  - --export-dir /folder/output
  - --update-mode allowinsert
  - --table product_recommendations

### Essentials

- Exchange data between DB and HDFS
- Tables are imported using MR jobs