# CCA-175: Chapter 7

### Working with RDD

- RDD?

  - Can hold any serializable type: 
    - Primitive types: integers, chars, booleans
    - Sequence types: strings, lists, arrays, tuples, dicts
    - POJO, POSO
  - Special types:
    - Pair RDD consists of KV pairs
    - Double RDD consists of numeric data

- How to create?

  - Create RDD from collection: sc.parallelize(collection)

    ```scala
    myData = ["Alice","Carlos","Frank","Barbara"]
    myRdd = sc.parallelize(myData)
    myRdd.take(2)
    // useful for testing, learning
    ```

  - Create RDD from files: sc.textFile(file-path|directory|wildcard-path|file-paths-comma-separated)

    ```scala
    sc.textFile("myfile.txt")
    sc.textFile("mydata/")
    sc.textFile("mydata/*.log")
    sc.textFile("myfile1.txt,myfile2.txt")
    ```

  - Create RDD from file in a specific URI: file:/home ... or hdfs://host/...

- textFile maps each line in a file to a separate RDD element using \n

- Input and Output Formats

  - Spark uses Hadoop InputFormat and OutputFormat Java classes
    - We can find many classes like this pattern XxxInputFormat and XxxOutputFormat 
      - Text, Sequence, FixedLength, AvroKey
      - Can implement its types
  - We can specify 
    - input format by using sc.hadoopFile or newAPIhadoopFile
      - textFile just calls hadoopFile
    - output format by using rdd.saveAsHadoopFile or saveAsNewAPIhadoopFile
      - saveAsTextFile calls saveAsHadoopFile

- sc.textFile: maps each line in a file to a separate RDD element

- sc.wholeTextFiles(dir): maps entire contents of each file in a directory to a single RDD element

### Essentials

- â€‹