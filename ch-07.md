# CCA-175: Chapter 7

### Working with RDD

- RDD?

  - Can hold any serializable type: 
    - Primitive types: integers, chars, booleans
    - Sequence types: strings, lists, arrays, tuples, dicts
    - POJO, POSO
  - Special types:
    - Pair RDD consists of KV pairs

    - Double RDD consists of numeric data

### How to create?

  - Create RDD from collection: sc.parallelize(collection)

    ```scala
    myData = ["Alice","Carlos","Frank","Barbara"]
    myRdd = sc.parallelize(myData)
    myRdd.take(2)
    // useful for testing, learning
    ```

  - Create RDD from files: sc.textFile(file-path|directory|wildcard-path|file-paths-comma-separated)

    ```scala
    sc.textFile("myfile.txt")
    sc.textFile("mydata/")
    sc.textFile("mydata/*.log")
    sc.textFile("myfile1.txt,myfile2.txt")
    ```

  - Create RDD from file in a specific URI: file:/home ... or hdfs://host/...

- textFile maps each line in a file to a separate RDD element using \n

- Input and Output Formats

  - Spark uses Hadoop InputFormat and OutputFormat Java classes
    - We can find many classes like this pattern XxxInputFormat and XxxOutputFormat 
      - Text, Sequence, FixedLength, AvroKey
      - Can implement its types
  - We can specify 
    - input format by using sc.hadoopFile or newAPIhadoopFile
      - textFile just calls hadoopFile
    - output format by using rdd.saveAsHadoopFile or saveAsNewAPIhadoopFile
      - saveAsTextFile calls saveAsHadoopFile

- sc.textFile: maps each line in a file to a separate RDD element

- sc.wholeTextFiles(dir): maps entire contents of each file in a directory to a single RDD element. Works only for small files

  ```
  /*
   * mydir = [f1.json = {"firstName":"Fred",..}, f2.json = {"firstName":"Barney",..}]
   */
  import json;
  myrdd1 = sc.wholeTextFiles(mydir); // each line is a file in this RDD.
  myrdd2 = myrdd1.map(lambda (fname,s): json.loads(s))
  for record in myrdd2.take(2): print record.get("firstName", None); // None == null
  // Fred, Barney
  ```


### Other General RDD Operations 

- Single-RDD Transformations

  - flatMap
  - distinct 
  - sortBy

  ```
  sc.textFile(file) // line1: the cat sat on the mat, line2: the aardvark sat on the sofa
  .flatMap(lambda line: line.split(' ')) // the cat ... 
  .distinct() // on mat sofa ...
  ```

- Multiple-RDD Transformations

  - intersection 
  - union 
  - zip
    - Creates value1, value2 pairs from each RDD element 
  - subtract 

- RDD operations

  - first
  - foreach
  - top(n): returns the largest n elements using natural ordering
  - sample: creates a new RDD with a sampling of elements
  - takeSample 
  - mean, sum, variance, stdev, 

### Essentials

- RDDs can be created from files, parallelized data in memory, or other RDDs
- sc.textFile reads newline-delimited text, one line per RDD element 
- sc.wholeTextFile reads multiple files, one file per RDD element
- Generic RDDs can consist of any type of data ยง Generic RDDs provide a wide range of transformation operations